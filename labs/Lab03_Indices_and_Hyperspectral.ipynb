{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03 - Vegetation Indices & KNN pixel classification\n",
    "\n",
    "Goals of the lecture:\n",
    "1. Apply different Vegetation Indices to multispectral satellite images (NDVI, NBR, NDSI)\n",
    "2. Conduct pixel-based classification on a hyperspectral image with a k-Nearest Neighbor (k-NN) classifier\n",
    "3. Evaluate the performance of the k-NN classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Vegetation Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "og8jYuadu0Pe"
   },
   "source": [
    "## Downloading Sentinel-2 image\n",
    "\n",
    "For convenience, we provide 11 bands of a 13-bands sentinel-2 patch. All bands are provided in a spatial resolution of 60mx60m to simplify index analysis and visualization (some bands were downscaled). The conversion to float32 and clipping have already been performed. The original band resolution can be seen below. **Please note**: in operational scenarios the lower resolution bands are usually upscaled to meet the spatial resolution of the 10mx10m bands. \n",
    "\n",
    "<img src=\"https://kai-tub.github.io/ben-docs/_images/ben_bands_vis.png\" align=\"center\" alt=\"Drawing\" style=\"width: 900px;height:470px\"/>\n",
    "\n",
    "Source: https://docs.kai-tub.tech/ben-docs/_images/ben_bands_vis.png\n",
    "\n",
    "Band Legend:\n",
    "- B04 is Red, B03 is Green, B02 is Blue\n",
    "- B05, B06, B07, B8A relate to VNIR (Visible Near Infrared)\n",
    "- B11, B12 relate to SWIR (Short Wave Infrared)\n",
    "- B01 band for Aerosols - _not needed in this lab_\n",
    "- B09 band for Water Vapor - _not needed in this lab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from tqdm.rich import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def download_file_with_progress(url: str, output_file: Path):\n",
    "    \"\"\"\n",
    "    Given a `url` as a String and an `output_file` as a file-path the item will\n",
    "    be downloaded and written to the `output_file`. If the `output_file` already\n",
    "    exists, it will be overwritten.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get(\"content-length\", 0))\n",
    "    chunk_size = 2**20  # mb\n",
    "\n",
    "    with open(output_file, \"wb\") as f:\n",
    "        for data in tqdm(\n",
    "            response.iter_content(chunk_size=chunk_size),\n",
    "            total=total_size // chunk_size,\n",
    "            unit=\"MB\",\n",
    "            unit_scale=True,\n",
    "            desc=\"Downloading\",\n",
    "        ):\n",
    "            f.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"./data\")\n",
    "data_path.mkdir(exist_ok=True)\n",
    "# For quick prototyping there is on such things as _too many_ asserts!\n",
    "assert data_path.exists, \"Should exist after calling mkdir!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_patch_path = data_path / \"river_patch.npy\"\n",
    "river_patch_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# download the preprosessed sub-image from homework 2\n",
    "\n",
    "download_file_with_progress(\n",
    "    \"https://tubcloud.tu-berlin.de/s/5ajRWDmaA6qAqTG/download/lab05_river_data.npz\", river_patch_path\n",
    ")\n",
    "river_patch = np.load(river_patch_path)\n",
    "river_patch.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "river_patch[\"B01\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = [\"B04\", \"B03\", \"B02\"]\n",
    "river_rgb = np.stack([river_patch[b] for b in channel_names], axis=-1)\n",
    "assert river_rgb.shape[-1] == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XBHwaQSXXzA"
   },
   "source": [
    "# Normalized Difference Vegetation Index\n",
    "\n",
    "As you learned in the theoretical lecture, the Normalized Difference Vegetation Index (NDVI) is a standard band-ratio calculation, which is used for ecological analysis and indicates whether the target area contains live green vegetation. NDVI is a normalized measure of the difference between reflectance at near-infrared and visible bands:\n",
    "\n",
    "NDVI = (NIR - Red) / (NIR + Red)\n",
    "\n",
    "Alternatively, VNIR band 8A can be used instead of NIR (B08). ESA recommends this procedure in its handbook for sentinel-2 satellite because narrower band 8A does not suffer from contamination due to water vapor ([page 12/64](https://sentinel.esa.int/documents/247904/685211/sentinel-2_user_handbook)). Generally, an NDVI value close or below 0 represents minimal or no greenness and a value close to 1 represents maximum greenness.\n",
    "\n",
    "Further reading: https://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/vegetation-indices-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lgoPIEND1Hs"
   },
   "outputs": [],
   "source": [
    "def calc_normalized_diff(b1: np.array, b2: np.array) -> np.array:\n",
    "    \"\"\"\n",
    "    Calculate the normalized difference over two n-dimensional numpy arrays.\n",
    "    src: https://earthpy.readthedocs.io/en/latest/_modules/earthpy/spatial.html\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b1, b2 : Two bands as numpy arrays\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    n_diff : (b1-b2) / (b1+b2) calculation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ignore warning for division by zero\n",
    "    with np.errstate(divide=\"ignore\"):\n",
    "        n_diff = (b1 - b2) / (b1 + b2)\n",
    "\n",
    "    # Set inf values to nan and provide custom warning\n",
    "    if np.isinf(n_diff).any():\n",
    "        warnings.warn(\n",
    "            \"Divide by zero produced infinity values that will be replaced \" \"with nan values\",\n",
    "            Warning,\n",
    "        )\n",
    "        n_diff[np.isinf(n_diff)] = np.nan\n",
    "\n",
    "    # Mask invalid values\n",
    "    if np.isnan(n_diff).any():\n",
    "        n_diff = np.ma.masked_invalid(n_diff)\n",
    "\n",
    "    return n_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quant_norm_data(data: np.ndarray, lower_quant: float = 0.01, upper_quant: float = 0.99) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the data by quantiles `lower_quant/upper_quant`.\n",
    "    The quantiles are calculated globally/*across all channels*.\n",
    "    \"\"\"\n",
    "    masked_data = np.ma.masked_equal(data, 0)\n",
    "    lq, uq = np.quantile(masked_data.compressed(), (lower_quant, upper_quant))\n",
    "    data = np.clip(data, a_min=lq, a_max=uq)\n",
    "    data = (data - lq) / (uq - lq)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rgb_vs_index(rgb_image: np.array, index_values: np.array, index_name: str):\n",
    "    \"\"\"\n",
    "    Plot the rgb image of a patch along with its calculated index values.\n",
    "    The index name determines the colormap in which the index values are computed.\n",
    "    \"\"\"\n",
    "    idx2cmap = {\"ndvi\": \"PiYG\", \"nbr\": \"PiYG\", \"ndsi\": \"jet\"}\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "    ax[0].axis(\"off\")\n",
    "    ax[0].set_title(\"RGB\")\n",
    "    ax[0].imshow(rgb_image)\n",
    "\n",
    "    ax[1].axis(\"off\")\n",
    "    ax[1].set_title(index_name.upper())\n",
    "    im = ax[1].imshow(index_values, cmap=idx2cmap[index_name], vmin=-1, vmax=1)\n",
    "\n",
    "    fig.subplots_adjust(right=0.85)\n",
    "    cbar_ax = fig.add_axes([0.90, 0.25, 0.02, 0.5])\n",
    "    fig.colorbar(im, cax=cbar_ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_band = river_patch[\"B04\"]\n",
    "vnir_band = river_patch[\"B8A\"]\n",
    "\n",
    "river_rgb = quant_norm_data(river_rgb)\n",
    "ndvi = calc_normalized_diff(vnir_band, red_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb_vs_index(river_rgb, ndvi, index_name=\"ndvi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZezWyn4XgEE"
   },
   "source": [
    "# Normalized Burn Ratio \n",
    "\n",
    "The Normalized burn ratio (NBR) is used to identify burned areas. Usually, you need two images acquired at different times (before and after the fire). The formula is similar to NDVI, except that it uses near-infrared and shortwave-infrared bands.\n",
    "\n",
    "NBR = (NIR - SWIR) / (NIR + SWIR)\n",
    "\n",
    "Also here, we will exchange NIR (B08) by VNIR band 8a. Values below 0 indicate burned areas.\n",
    "\n",
    "Further information: https://www.earthdatascience.org/courses/use-data-open-source-python/multispectral-remote-sensing/vegetation-indices-in-python/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swir_band = river_patch[\"B11\"]\n",
    "\n",
    "nbr = calc_normalized_diff(vnir_band, swir_band)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb_vs_index(river_rgb, nbr, \"nbr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A closer look actually reveals some burned areas in the amazon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rgb_vs_index(river_rgb[1200:1700, 1100:1600], nbr[1200:1700, 1100:1600], \"nbr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbCYo3cC0l6m"
   },
   "source": [
    "# In-course practice I (Normalized Difference Snow Index)\n",
    "\n",
    "The Normalized Difference Snow Index (NDSI) is an index that shows the presence of snow in a pixel. The formula is similar to NDVI, and NBR except that it uses green and shortwave-infrared bands:\n",
    "\n",
    "NDSI = (Green - SWIR) / (Green + SWIR)\n",
    "\n",
    "The range between 0 and 1 can be associated with snow. Any value below 0 indicates the absence of snow.\n",
    "\n",
    "**Task**: To do this practice, we select another patch which includes snow. The following cell will automatically download the patch. It is preprocessed in the same way as the river patch is. Compute the NDSI for the the snowpatch. Select and quantile-normalize the RGB channels for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_patch_path = data_path / \"snow_patch.npy\"\n",
    "snow_patch_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file_with_progress(\n",
    "    \"https://tubcloud.tu-berlin.de/s/fmbBqbojf3GA7Em/download/lab05_snow_data.npz\", snow_patch_path\n",
    ")\n",
    "snow_patch = np.load(snow_patch_path)\n",
    "snow_patch.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXk9fyK51QyC"
   },
   "source": [
    "# Part II: Pixel-Based Supervised Classification with K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this course we use the \"IndianPines\" hyperspectral image selected from Hyperspectral (HS) datasets that are publicly available at http://www.ehu.eus/ccwintco/index.php/Hyperspectral_Remote_Sensing_Scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oz6qYgnalqXu"
   },
   "source": [
    "## Define Dataset class\n",
    "\n",
    "This notebook downloads the \"IndianPines\" hyperspectral image and its corresponding ground reference map (class labels assigned per pixel). \n",
    "\n",
    "- 200 bands\n",
    "- 16 land cover classes, namely: 'Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn', 'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', 'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill', 'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives', 'Stone-Steel-Towers'.\n",
    "\n",
    "**Note:** that in the classification map some areas are not labeled ('Undefined') that have to be excluded.\n",
    "\n",
    "Download also available via TUB-cloud: https://tubcloud.tu-berlin.de/s/oAdtGjHzTTJH7Xc/download/IndianPines.zip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndianPines():\n",
    "    def __init__(self, base_dir: str):\n",
    "    \n",
    "        self.download_dataset(base_dir)\n",
    "        self.datacube = self.normalize_img(self.open_datacube(base_dir))\n",
    "        self.gt = self.open_gt(base_dir)\n",
    "\n",
    "        self.class_names = [\n",
    "            \"Undefined\",\n",
    "            \"Alfalfa\",\n",
    "            \"Corn-notill\",\n",
    "            \"Corn-mintill\",\n",
    "            \"Corn\",\n",
    "            \"Grass-pasture\",\n",
    "            \"Grass-trees\",\n",
    "            \"Grass-pasture-mowed\",\n",
    "            \"Hay-windrowed\",\n",
    "            \"Oats\",\n",
    "            \"Soybean-notill\",\n",
    "            \"Soybean-mintill\",\n",
    "            \"Soybean-clean\",\n",
    "            \"Wheat\",\n",
    "            \"Woods\",\n",
    "            \"Buildings-Grass-Trees-Drives\",\n",
    "            \"Stone-Steel-Towers\",\n",
    "        ]\n",
    "        self.ignored_labels = [0]\n",
    "        self.rgb_bands = (31, 15, 11)\n",
    "        self.num_bands = self.datacube.shape[-1]\n",
    "        self.num_classes = len(self.class_names)\n",
    "    \n",
    "    def download_dataset(self, base_dir: str):\n",
    "        indian_pines_path = base_dir / \"IndianPines.zip\"\n",
    "        download_file_with_progress(\n",
    "            \"https://tubcloud.tu-berlin.de/s/oAdtGjHzTTJH7Xc/download/IndianPines.zip\",\n",
    "            indian_pines_path,\n",
    "        )\n",
    "        zipf = zipfile.ZipFile(indian_pines_path)\n",
    "        zipf.extractall(base_dir)\n",
    "    \n",
    "    def open_datacube(self, base_dir: str) -> np.array:\n",
    "        image_path = base_dir /  \"IndianPines/Indian_pines_corrected.mat\"\n",
    "        return scipy.io.loadmat(image_path)[\"indian_pines_corrected\"]\n",
    "    \n",
    "    def open_gt(self, base_dir: str) -> np.array:\n",
    "        gt_path = base_dir / \"IndianPines/Indian_pines_gt.mat\"\n",
    "        return scipy.io.loadmat(gt_path)[\"indian_pines_gt\"]\n",
    "    \n",
    "    def normalize_img(self, img: np.array) -> np.array:\n",
    "        img = np.asarray(img, dtype=\"float32\")\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img))\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_pines = IndianPines(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_pines.datacube.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indian_pines.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HblrhDORmT1m"
   },
   "source": [
    "## Visualization of the Image and Land Cover Map (Classification Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import spectral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create handles for legend\n",
    "cmap = matplotlib.colormaps.get_cmap(\"tab20\")\n",
    "handles = [matplotlib.patches.Patch(color=cmap(i), label=indian_pines.class_names[i]) for i in range(17)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_cmap(segmentation_map:np.array, cmap_name: str) -> matplotlib.colors.ListedColormap:\n",
    "    \"Adjust color pallette according to classes present in segmentation map.\"\n",
    "    cmap = matplotlib.colormaps.get_cmap(cmap_name)\n",
    "    colors = cmap(np.linspace(0, 1, cmap.N))\n",
    "    adj_colors = colors[:np.max(segmentation_map) + 1]\n",
    "    return matplotlib.colors.ListedColormap(adj_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube_rgb = spectral.get_rgb(indian_pines.datacube, bands=indian_pines.rgb_bands)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 7))\n",
    "axes[0].set_title(\n",
    "    \"Hyperspectral Image ({}x{}x{})\".format(*indian_pines.datacube.shape), fontsize=15, pad=20\n",
    ")\n",
    "axes[0].imshow(datacube_rgb)\n",
    "axes[1].set_title(\"Ground Truth Ref. Map ({}x{})\".format(*indian_pines.gt.shape), fontsize=15, pad=20)\n",
    "axes[1].imshow(indian_pines.gt, cmap=adjust_cmap(indian_pines.gt, \"tab20\"), interpolation=\"none\")\n",
    "axes[1].legend(handles=handles, loc=(1.04, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKjgv45Emcub"
   },
   "source": [
    "## Creating Train and Test sets\n",
    "\n",
    "We need to create two sets for train and test purposes.\n",
    "\n",
    "In the pixel-based classification each set (test/train) includes a certain number of samples. Each sample represents a pixel with its spectral values and its assigned class label. For the IndianPines dataset each sample consists of 200 spectral band values and one label (between 0-16)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_gt(gt: np.array, train_percentage: float) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Extract a fixed percentage of samples from an 2d-array of labels.\n",
    "\n",
    "    Parameter\n",
    "    ---------\n",
    "    gt                : a 2D array of int labels\n",
    "    train_percentage  : [0, 1] float\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    train_gt, test_gt : 2D arrays of int labels\n",
    "\n",
    "    \"\"\"\n",
    "    train_gt, test_gt = gt.copy(), gt.copy()\n",
    "\n",
    "    for c in np.unique(gt):\n",
    "        mask = gt == c\n",
    "        for x in range(gt.shape[0]):\n",
    "            first_half_count = np.count_nonzero(mask[:x, :])\n",
    "            if first_half_count / np.count_nonzero(mask) > train_percentage:\n",
    "                break\n",
    "\n",
    "        train_mask = mask.copy()\n",
    "        train_mask[:x, :] = 0\n",
    "        train_gt[train_mask] = 0\n",
    "\n",
    "        test_mask = mask.copy()\n",
    "        test_mask[x:, :] = 0\n",
    "        test_gt[test_mask] = 0\n",
    "\n",
    "    return train_gt, test_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt, test_gt = sample_gt(indian_pines.gt, 0.8)\n",
    "\n",
    "n_train = np.count_nonzero(train_gt)\n",
    "n_test = np.count_nonzero(test_gt)\n",
    "n_total = np.count_nonzero(indian_pines.gt)\n",
    "\n",
    "print(\"Train:  {} samples selected (over {})\".format(n_train, n_total))\n",
    "print(\"Test:   {} samples selected (over {})\".format(n_test, n_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 7))\n",
    "axes[0].set_title(\"Train Set\", fontsize=15, pad=20)\n",
    "axes[0].imshow(train_gt, cmap=adjust_cmap(indian_pines.gt, \"tab20\"), interpolation=\"none\")\n",
    "axes[1].set_title(\"Test Set\", fontsize=15, pad=20)\n",
    "axes[1].imshow(test_gt, cmap=adjust_cmap(indian_pines.gt, \"tab20\"), interpolation=\"none\")\n",
    "axes[1].legend(handles=handles, loc=(1.04, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pixel_classification_dataset(\n",
    "    hypercube: np.array, gt: np.array, ignored_labels: List[int] = []\n",
    ") -> (np.array, np.array):\n",
    "    \"\"\"Based on ground truth reference map, divide hypercube into individual hyperspectral pixel datapoints.\"\"\"\n",
    "    assert hypercube.shape[:2] == gt.shape[:2]  # spatial dimension of hypercube and ground truth equal?\n",
    "    X, y = [], []\n",
    "    classes = set(np.unique(gt)) - set(ignored_labels)\n",
    "\n",
    "    for cl in classes:\n",
    "        # cl_indices is a tuple of two np.array, one for the x and and one for y coordinate\n",
    "        cl_indices = np.nonzero(gt == cl)\n",
    "        X += list(hypercube[cl_indices])\n",
    "        y += len(cl_indices[0]) * [cl]\n",
    "\n",
    "    # shuffle data\n",
    "    xy_zipped = list(zip(X, y))\n",
    "    np.random.shuffle(xy_zipped)\n",
    "    X, y = zip(*xy_zipped)\n",
    "\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data\n",
    "X_train, y_train = create_pixel_classification_dataset(\n",
    "    indian_pines.datacube,\n",
    "    train_gt,\n",
    "    indian_pines.ignored_labels\n",
    ")\n",
    "\n",
    "# create testing data\n",
    "X_test, y_test = create_pixel_classification_dataset(\n",
    "    indian_pines.datacube,\n",
    "    test_gt,\n",
    "    indian_pines.ignored_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEeMMfaxmxhh"
   },
   "source": [
    "## In-course practice II\n",
    "\n",
    "Plot spectral signature of 3 different classes. These are the features that we use for classification.\n",
    "You can select one single sample or calculate the average of samples for each class from train samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1621544888250,
     "user": {
      "displayName": "Mat Rb",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gik24jvA_yZMp2z50oFh_gbhSlIRnt_v5WBVKY6REE=s64",
      "userId": "17685210069082378191"
     },
     "user_tz": -120
    },
    "id": "e-eyD-cnmzny",
    "outputId": "77ff48f2-56e3-48c1-bae9-54785f8d474c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbBARIrDm57H"
   },
   "source": [
    "## Training a KNN Classifer\n",
    "\n",
    "k-Nearest Neighbor is a supervised algorithm, which is usually used for classification tasks. It measures the distances between the training samples (shown by circles in the figure) and unseen test data (is shown by *star* in the figure) and makes the prediction (class label) accordingly.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*2zYNhLc522h0zftD1zDh2g.png\" align=\"left\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "credit: medium.com/@equipintelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize knn model with n_neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "# fit (train) the knn model with our trainings data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a trained KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/400px-Precisionrecall.svg.png\" align=\"right\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "<br> <br>\n",
    " \n",
    "<font size=\"4\"> <ins> **Evaluation Metrics** </ins> </font>\n",
    " \n",
    "\n",
    "<font size=\"4\"> TP = classified correctly as True (sample belongs to class)  </font>\n",
    "\n",
    "<font size=\"4\"> TN = classified correctly as False (sample belongs not to class)  </font>\n",
    "\n",
    "<font size=\"4\"> FP = classified incorrectly as True (sample belongs not to class)  </font>\n",
    "\n",
    "<font size=\"4\"> FN = classified incorrectly as False (sample belongs to class)  </font>\n",
    "\n",
    "<font size=\"4\"> N = TP + TN + FP + FN  </font>\n",
    "\n",
    "<font size=\"4\"> Accuracy = (TP + TN) / N  </font>\n",
    "\n",
    "<font size=\"4\"> Precision = TP / (TP + FP)  </font>\n",
    "\n",
    "<font size=\"4\"> Recall = TP (TP + FN)  </font>\n",
    "\n",
    "<font size=\"4\"> F1-Score = 2 x (Precision x Recall) / (Precision + Recall)  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.count_nonzero(y_pred == y_test) / len(y_pred)\n",
    "print(\"Accuracy for k=1: {:.2f}\".format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_cl_ints = [i for i in range(len(indian_pines.class_names)) if not i in indian_pines.ignored_labels]\n",
    "active_cl_names = [indian_pines.class_names[i] for i in active_cl_ints]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, y_pred, target_names=active_cl_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sl6DykvanQGW"
   },
   "source": [
    "## Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, 1, figsize=(11, 7))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    cmap=\"viridis\",\n",
    "    fmt=\"d\",\n",
    "    xticklabels=active_cl_names,\n",
    "    yticklabels=active_cl_names,\n",
    "    annot=True,\n",
    ")\n",
    "plt.xlabel(\"True Label\", fontsize=15)\n",
    "plt.ylabel(\"Predicted Label\", fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30DD8CCrnXsF"
   },
   "source": [
    "## In-course practice III\n",
    "\n",
    "* Reduce the number of training samples into 2% and observe how the accuracy changes\n",
    "\n",
    "* Instead of 1-NN, use 3-NN and 7-NN for KNN classifier and observe how the accuracy changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNlAzZx05usoBjQmVnEH/cQ",
   "name": "Lab05.ipynb",
   "provenance": [
    {
     "file_id": "1Ewv3AhusIg6q02cB16RfIMtKGspQiZ3s",
     "timestamp": 1621545209784
    },
    {
     "file_id": "1ZIhpBuck1eHxIk_KZbeKsVrH7qHEr7He",
     "timestamp": 1591548592342
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ipl4eo_updated",
   "language": "python",
   "name": "ipl4eo_updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
